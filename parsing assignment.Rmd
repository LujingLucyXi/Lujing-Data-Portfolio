---
title: 'Constituency and Dependency Parsing'
author: "Lujing Xi"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
#do not change this
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries / R Setup

- In this section, include the *R* set up for Python to run. 

```{r}
##r chunk
library(reticulate)
py_config()
```

- In this section, include import functions to load the packages you will use for Python.

```{python}
##python chunk
import nltk
import spacy
from spacy import displacy

from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
```

## Import the grammar

- Modify `grammar1` from the lecture notes to account for the following sentences:
  - New sentence: "The dog ate the food."
  - New sentence: "The dog walked by the cat in the park." 
  
```{python}
##python chunk
grammar1 = nltk.CFG.fromstring("""
  S -> NP VP
  VP -> V NP | V NP PP
  PP -> P NP
  V -> "saw" | "ate" | "walked"
  NP -> "John" | "Mary" | "Bob" | Det N | Det N PP
  Det -> "a" | "an" | "the" | "my"  | "The"
  N -> "food" | "dog" | "cat" | "telescope" | "park"
  P -> "in" | "on" | "by" | "with"
  """)
```

## Process the sentences

- Process the sentences with both the `RecursiveDescentParser` and `ShiftReduceParser`.

```{python}
##python chunk
rd_parser = nltk.RecursiveDescentParser(grammar1)
sent1 = "The dog ate the food".split()
print(sent1)


for tree1 in rd_parser.parse(sent1):
     print(tree1)

sent2 = "The dog walked by the cat in the park".split()
print(sent2)     

for tree2 in rd_parser.parse(sent2):
     print(tree2)



sr_parser = nltk.ShiftReduceParser(grammar1)
sent3 = "The dog ate the food".split()
print(sent3)


for tree3 in sr_parser.parse(sent3):
     print(tree3)
 
sent4 = "The dog walked by the cat in the park".split()
print(sent4)

for tree4 in sr_parser.parse(sent4):
     print(tree4) 
```

## Training Data

- Use *two* of your tweets from the previous assignment and modify the training data for dependency parsing. 

```{python}
##python chunk 
nlp = spacy.load('en_core_web_sm')
sentence1 = "I will be the best by far in fighting terror"
sentence1_nlp = nlp(sentence1)
for token in sentence1_nlp:
    print("{0}/{1} <--{2}-- {3}/{4}".format(
        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))
        
        
displacy.render(sentence1_nlp,
                options={'distance': 110,
                         'arrow_stroke': 2,
                         'arrow_width': 8})
                                 
sentence2 = "I am in Las Vegas at the best hotel Trump International"
sentence2_nlp = nlp(sentence2)
for token in sentence2_nlp:
    print("{0}/{1} <--{2}-- {3}/{4}".format(
        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))
        
        
        
        
displacy.render(sentence2_nlp,
                options={'distance': 110,
                         'arrow_stroke': 2,
                         'arrow_width': 8})   

        

train_data = [
    ("I will be the best by far in fighting terror", 
        { 
       'heads': [2,2,2,4,2,5,2,7,8,9],
       
        'deps': ['nsubj', 'aux','ROOT', 'det', 'attr', 'advmod','advmod', 'prep',
        'pcomp','dobj'] 
    } 
    ), 
    ("I am in Las Vegas at the best hotel Trump International", 
    { 
        'heads': [1,1,1,3,2,1,6,7,8,9,9],
       
        'deps': ['nsubj', 'ROOT', 'prep', 'compound', 'pobj', 'prep','det','amod','pobj','compound','appos'] 
    } 
    )
]          
```


## Build the model

- Create a blank spacy pipeline.
- Add the parser to the pipeline.
- Add the labels to the pipeline.

```{python}
##python chunk
nlp = spacy.blank("en")

# add the parser
parser = nlp.create_pipe("parser")
nlp.add_pipe(parser,first=True)

# add the labels
for _, annotations in train_data:
        for dep in annotations.get('deps', []):
            parser.add_label(dep)
```

## Train the model

- Train the model with 10 iterations of the data. 

```{python}
##python chunk
optimizer = nlp.begin_training()
n_iter = 10
#run training
for itn in range(n_iter):
    random.shuffle(train_data)
    losses = {}
    for text, annotations in train_data:
        nlp.update([text], [annotations], sgd=optimizer, losses=losses)
    print(losses)
```

## Test the model

- Test your dependency model on a similar tweet.

```{python}
##python chunk
test_text = "I am at Trump National Doral-best resort in U.S."
doc = nlp(test_text)
print('Dependencies', [(t.text, t.dep_, t.head.text) for t in doc])
```

## Visualize

- Include a visualization of the tweet you just tested. 
- Remember, you should modify the chunk options to show the picture in the knitted document, since it does not display inline. 

```{python}
##python chunk
displacy.render(doc, style="dep",
                options={'distance': 110,
                         'arrow_stroke': 2,
                         'arrow_width': 8})
```